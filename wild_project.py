# -*- coding: utf-8 -*-
"""Wild project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MCC3RryNC_8wj2rxDjDF-N6Kh_E3Xugn
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

#load the data
df = pd.read_csv('/content/Shill Bidding Dataset.csv')

#undersatnd the dataset structure
print(df.info())
print(df.isnull().sum()) #searching for missing values

#sta summary
print(df.describe())

#class distribution of numerical features
class_count = df['Class'].value_counts() #to check how many unique value in the class
print(class_count)
#bar plot to show the distributions of the classes
plt.bar(class_count.index, class_count.values)
plt.title('Class distribution')
plt.xticks([0, 1], ['non-Shill', 'Sill'])
plt.ylabel('Frequency')
plt.show()

#distribution of numerical features
numerical_features=['Bidder_Tendency', 'Bidding_Ratio', 'Successive_Outbidding',
                    'Last_Bidding', 'Auction_Bids', 'Starting_Price_Average',
                    'Early_Bidding', 'Winning_Ratio', 'Auction_Duration']

fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(20,20))
fig.tight_layout(pad=5.0)

#loop throw the features
for i, ax in enumerate(axes.flatten()): #this is to avoid error
  if i < len(numerical_features):
    sns.histplot(df[numerical_features[i]], bins=50, ax=ax, kde=True)
    ax.set_title(f'{numerical_features[i]} distribution')
  else:
    ax.set_visible(False) #hiding the unused subplots

plt.show()

#analys the distrivbution of the ratio of bidding
plt.figure(figsize=(10,4))
sns.histplot(df[df['Class']==1]['Bidding_Ratio'], bins=50, color='red', label='Shill') #here plote the class 1 also we filter it to include only 1 (the rows)
sns.histplot(df[df['Class']==0]['Bidding_Ratio'], bins=50, color='blue', label='Non-Shill') # this is for class 0
plt.xlabel('Bidding_Ratio')
plt.ylabel('Frequency')
plt.legend()
plt.show()

df.corr()

#correlation

corr_matrix=df.corr()
plt.figure(figsize=(12,10))
sns.heatmap(corr_matrix, cmap='coolwarm_r', annot=True, fmt='2f') # to show only 2 decimal
plt.title('correlation matrix')
plt.show()

#outliers
import numpy as np

outlier_fraction = 0.01 # define the fraction of data points
#to find the number of the standard divation from the mean
z_scores= np.abs(df[['Bidding_Ratio','Successive_Outbidding']] - df[['Bidding_Ratio', 'Successive_Outbidding']].mean())/ df[['Bidding_Ratio','Successive_Outbidding']].std()
#detrmine the outliers with  z score in top of %1 of all the score
outliers = z_scores > z_scores.quantile(1- outlier_fraction)

plt.figure(figsize=(10,4))
plt.scatter(df['Bidding_Ratio'], df['Successive_Outbidding'], c= outliers['Bidding_Ratio'] | outliers['Successive_Outbidding'], cmap='viridis')
plt.title('outliers in Bidding_Ratio v Successive_Outbidding')
plt.xlabel('Bidding_Ratio')
plt.ylabel('Successive_Outbidding')
plt.show()

from sklearn.preprocessing import StandardScaler
import pandas as pd


#transfer non numeric to numeric with dummy
df_encoded = pd.get_dummies(df,columns=['Bidder_ID'], drop_first=True)


scaler= StandardScaler()
X_scaled = scaler.fit_transform(df_encoded.drop('Class', axis=1))
#trying to reduce the dimintionalty to 2d
pca = PCA(n_components=2)
#now pca trandfromation to scaled features
X_pca =pca.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df['Class'], palette='viridis')
plt.title('pca results')
plt.xlabel('pca1')
plt.ylabel('pca2')
plt.legend(title='Class')
plt.show()

#reduce the dimintilaty for visualizations with tsne


tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=df_encoded['Class'])
plt.title('t-SNE')
plt.xlabel('t-SNE for feature1')
plt.ylabel('t-SNE for feature2')
plt.legend(title='Class')
plt.show()

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV


#as you see spliting the data
x_train, x_test, y_train, y_test = train_test_split(X_scaled, df['Class'], test_size=0.2, random_state=42)

#set up the svm parameters
svm = SVC( probability=True)

#set up the parameters
param_grid= {
    'C':[0.1, 10],
    'gamma':[ 0.001, 0.1 ],
    'kernel':['rbf']
  }

#preform it with the grid using 5 fold cross validation
grid_search=GridSearchCV(svm, param_grid, cv=3,n_jobs=-1)
grid_search.fit(x_train, y_train)


#picking up the best estimator
best_svm= grid_search.best_estimator_
print('best parmeter', grid_search.best_params_)
print('best estimator', best_svm)

#best score achived during the gride search
best_score =grid_search.best_score_
print('best score (mean cross validation score of the best estiamtor):', best_score)

from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc

#prediction on the test set
y_pred= grid_search.predict(x_test)

#classifcation report
print(classification_report(y_test, y_pred))

#confusion matrix
print(confusion_matrix(y_test, y_pred))

#precison recall curve
precisoin, recall, _ = precision_recall_curve(y_test, grid_search.decision_function(x_test))
#auc measure
auc_precisoin_recall=auc(recall, precisoin)

print(f'auc of precision-recall curve:{auc_precisoin_recall}')

"""#Suggestions to enhance the data work and results by trying to:
- handling the outliers
-applying tsne within svm : didn't work  so i had to use Randomforestclassifier
-reduceing the noise
-fairness consideration
-roc curve
"""

#handling the class imbalance with smote
from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_smote, y_smote= smote.fit_resample(X_scaled, df['Class'])

#outlier detection and removal

from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import RFECV
iso_forest=IsolationForest( n_estimators = 100, contamination = 'auto', max_samples='auto', random_state=42)
outliers_detected =iso_forest.fit_predict(X_smote)== -1
X_clean, y_clean = X_smote[~outliers_detected], y_smote[~outliers_detected]

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
#noise reduction with feature selection
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

rf.fit(X_clean, y_clean)


selector = SelectFromModel(rf, prefit=True, threshold='mean',)

X_selected = selector.fit_transform(X_clean)

#svm with class weight and hyperarameter tungin
svm_balanced= SVC(probability=True, class_weight='balanced', random_state=42)
param_grid_balanced= {'C': [1, 10], 'gamma': [0.001, 0.1], 'kernel': ['rbf']}
grid_search_balanced =GridSearchCV(svm_balanced, param_grid_balanced,cv=3, n_jobs=-1, verbose=3)
grid_search_balanced.fit(x_train,y_train)
print('best parameter balanced', grid_search_balanced.best_params_)
print('best estimator balanced', grid_search_balanced.best_estimator_)
print('best estimator balanced', grid_search_balanced.best_score_)

y_pred_balanced = grid_search_balanced.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve

#additional roc curve
decision_scores = grid_search_balanced.decision_function(x_test)
fpr, tpr, thresholds= roc_curve(y_test, decision_scores)
plt.figure(figsize=(10,8))
plt.plot(fpr,tpr,label=f'roc curve(area={roc_auc_score(y_test, decision_scores): 0.2f})')
plt.plot([0, 1], [0,1], 'k--')
plt.xlabel('false positive rate')
plt.ylabel('true psoitive rate')
plt.title('roc curve')
plt.legend(loc='lower right')
plt.show()